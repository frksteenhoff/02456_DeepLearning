{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkgreen\">Course Notes - 02456 - Deep Learning</span>\n",
    "### <span style=\"color:darkgreen\">Lecturer: Ole Winther</span>\n",
    "Special course spring 2018\n",
    "\n",
    "----\n",
    "\n",
    "## <span style=\"color:darkgreen\">Basics</span>\n",
    "* Outline of entire course found in this Google Docs: [Course plan and information](https://docs.google.com/document/d/1C082chAd3ZX23jQG8HthO2LQB9bMe_-otPtPpBGRScI/edit)\n",
    "* Course material found on YouTube: [02456 - Deep Learning](https://www.youtube.com/channel/UCCrKU0xGX5DzBfVsZsFNesQ)\n",
    "* Additional material from Hugo Larochelle on YouTube: [Neural Networks Class - Universit√© de Sherbooke](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)\n",
    "\n",
    "\n",
    "* Book used in course: Michael Nielsen, [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\n",
    "* Alternative book: Goodfellow, Bengio and Courville, [Deep Learning](http://www.deeplearningbook.org/)\n",
    "* Link to course slides [here](https://drive.google.com/open?id=0BxJRy96AHCJxWWRVTzMwd05uakU)\n",
    "<img src=\"DataScience.png\" alt=\"The skills of a Data Scientist\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## <span style=\"color:darkgreen\">Week 1</span>\n",
    "#### <span style=\"color:darkgreen\">Topics</span> \n",
    "> What is deep learning and statistical artificial intelligence, feed-forward neural network (FFNN), training with back propagation, optimisation\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkgreen\">Definitions</span>\n",
    "\n",
    "#### `Singularity` \n",
    "Going beyond human intelligence. Bye bye Moore's law.\n",
    "> The hypothesis that the invention of artificial superintelligence will abruptly trigger runaway technological growth, resulting in unfathomable changes to human civilization. According to this hypothesis, an upgradable intelligent agent (such as a computer running software-based artificial general intelligence) would enter a \"runaway reaction\" of self-improvement cycles, with each new and more intelligent generation appearing more and more rapidly, causing an intelligence explosion and resulting in a powerful superintelligence that would, qualitatively, far surpass all human intelligence. [Wiki1](https://en.wikipedia.org/wiki/Technological_singularity)\n",
    "\n",
    "\n",
    "\n",
    "#### `Reinforcement learning`\n",
    "> How software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. [Wiki2](https://en.wikipedia.org/wiki/Reinforcement_learning)\n",
    "\n",
    "#### `Deep learning`\n",
    "Rebranding of Artificial Neural Network. It is deep if it has more than one layer of non-linear feature transformation.\n",
    "\n",
    "> Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have drastically improved the state-of-the-art in speech recongition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate sstructure in large data sets by using backpropagation algorithm to indicare how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. \n",
    "\n",
    "#### `Regularization` \n",
    "Poor man's Bayes Inference\n",
    "\n",
    "#### `Adam algorithm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### <span style=\"color:darkgreen\">Lecture content</span>\n",
    "\n",
    "#### Major areas in AI\n",
    "Specialized AI -- working within a certain scope.\n",
    "<img src=\"AI-areas.png\" alt=\"How far we have come with AI\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "#### Progress within deep learning in the last years\n",
    "* 2014 Google acquired DeepMind\n",
    "* Deep learning in self-driving cars is most disruptive\n",
    "* Speech recognition breakthrough with deep learning\n",
    "* Image classification\n",
    "\n",
    "<img src=\"image-class.png\" alt=\"Image classification\" style=\"width: 400px; float:left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual features\n",
    "Latent representations - modifying visual features\n",
    "\n",
    "#### Representation learning\n",
    "**Traditional**: Data -> Feature engineering -> Machine learning\n",
    "* Feature selection\n",
    "* Feature extraction (PCA etc.)\n",
    "* Feature construction (SIFT etc.)\n",
    "\n",
    "**With deep learning**: Data -> End-to-end learning\n",
    "\n",
    "Image to classification all in one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward neural networks\n",
    "<img src=\"ffnn.png\" alt=\"Feed forward neural network\" style=\"width: 500px;\"/>\n",
    "\n",
    "One hidden layer, weights, apply some sort of non linearity (activation layer), compute output.\n",
    "\n",
    "<img src=\"func.png\" alt=\"Non-linearity and training - activation functions\" style=\"width: 500px;\"/>\n",
    "\n",
    "the hyperbolic tangent (`tanh`) was very popular, but gradient descent can get stuck using this method. Instead the rectified linear unit (`relu`) is used now.\n",
    "\n",
    "#### Basic idea:\n",
    "Define loss function and minimize that with stochastic gradient descent\n",
    "\n",
    "<img src=\"grad-desc.png\" alt=\"Gradient Descent\" style=\"width: 250px; float:left;\"/>\n",
    "\n",
    "Gradient descent - take small steps in the oposite direction of the gradient towards your minimum. This can either be done with regular gradient descent (upper picture) or stochastic (random) gradient descent, where the path looks a bit more fuzzy. This is based on mini-batch learning which has proven to be the most correct solution, where you compute the gradient on a subset of your training data.\n",
    "\n",
    "### `Activation functions (cost function)`\n",
    "* Rectified linear unit (negative contribution are set to 0, which means that in a layer using relu only the positive contributions will be seen in output)\n",
    "* Softmax (good for classification output is a probability, will sum to one, non-negative)\n",
    "* Hyperbolic tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity\n",
    "Sometimes units become completely dead.\n",
    "\n",
    "### Neural network training - backpropagation\n",
    "#### `Backpropagation (stochastic gradient descent)`\n",
    "\n",
    "#### Training criterion\n",
    "Cost function - connects to normal statistics, minus log likelihood function. Probability of y given x, we are only interested in y.\n",
    "\n",
    "We have seen a lot of data. We want to maximize the probability of seeing this data.\n",
    "\n",
    "\n",
    "**`One hot encoding`** - taking a vector of classes and making that into sparse vector, same dimensionalty as number of classes.\n",
    "\n",
    "### Optimization\n",
    "Step size (learning rate)\n",
    "\n",
    "**Mini-batch training**\n",
    "* No need to have an accurate estimate of `g` (initial parameter setting)\n",
    "* Use only small batch of training data at once\n",
    "* Many updates per epoch (seeing data once)\n",
    "* Need to update steps size through process\n",
    "\n",
    "----\n",
    "### <span style=\"color:darkgreen\">Questions</span>\n",
    "* What do you mena by saying that regularization is the poor man's version of Bayes' Inference, could you elaborate on that?\n",
    "* How do you choose the number of hidden units and layers in your network?\n",
    "* Could you explain the saving of computations in backpropagation in more detail=\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## <span style=\"color:darkgreen\">Week x</span>\n",
    "#### <span style=\"color:darkgreen\">Topics</span> \n",
    "> \n",
    "\n",
    "### <span style=\"color:darkgreen\">Definitions</span>\n",
    "----\n",
    "### <span style=\"color:darkgreen\">Lecture content</span>\n",
    "----\n",
    "### <span style=\"color:darkgreen\">Questions</span>\n",
    "----\n",
    "### <span style=\"color:darkgreen\">Exercises</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
